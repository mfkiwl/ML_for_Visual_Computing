{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON-LINEAR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  All algorithms were designed by Hyungjoo Kim and Dataset was provided at UCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.load('./IRIS/iris_train_labels.npy') # [2 1 1 0 2 2 2 0 1...] (96,)\n",
    "train_samp = np.load('./IRIS/iris_train_samples.npy') # (96,4) sepal length, sepal width, petal length, petal width.\n",
    "val_label = np.load('./IRIS/iris_val_labels.npy')\n",
    "val_samp = np.load('./IRIS/iris_val_samples.npy')\n",
    "# Class\n",
    "# Iris Setosa for label 0\n",
    "# Iris Versicolour label 1\n",
    "# Iris Virginica for label 2\n",
    "MNIST_train_label = np.load('./MNIST/mnist_train_labels.npy')\n",
    "MNIST_train_samp = np.load('./MNIST/mnist_train_samples.npy')  # (44800, 28*28)\n",
    "MNIST_val_label = np.load('./MNIST/mnist_val_labels.npy')\n",
    "MNIST_val_samp = np.load('./MNIST/mnist_val_samples.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Implement classification based on logistic regression using GD by implementing the gradient function deLogistic(preds, X, Y) and optimizing using GD. preds are the prediction from the model, X are the data and Y are the labels. Propose a function interface for your implementation of the gradient descent algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 1000 Current loss is 0.29210562276506374, Traning accuracy is 0.6666666666666666\n",
      "200 / 1000 Current loss is 0.2279424166160572, Traning accuracy is 0.6666666666666666\n",
      "300 / 1000 Current loss is 0.1527039094729769, Traning accuracy is 0.6666666666666666\n",
      "400 / 1000 Current loss is 0.10864737225952444, Traning accuracy is 0.6770833333333334\n",
      "500 / 1000 Current loss is 0.08956502524958765, Traning accuracy is 0.9479166666666666\n",
      "600 / 1000 Current loss is 0.07984472434881174, Traning accuracy is 0.9791666666666666\n",
      "700 / 1000 Current loss is 0.07328503200342673, Traning accuracy is 0.9895833333333334\n",
      "800 / 1000 Current loss is 0.0680390364333412, Traning accuracy is 1.0\n",
      "900 / 1000 Current loss is 0.06351621127950516, Traning accuracy is 1.0\n",
      "1000 / 1000 Current loss is 0.05948988217387365, Traning accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def loss_function(preds, Y):\n",
    "    return np.sum((preds - Y)**2) / len(Y)\n",
    "\n",
    "def de_Logistic(preds, X, Y):\n",
    "    dw = (1 / len(Y)) * np.matmul(X.T, preds - Y)\n",
    "    db = (1 / len(Y)) * np.sum(preds - Y)\n",
    "    return dw, db\n",
    "\n",
    "def Logistic_train_classifier(X, Y, learning_rate, max_iter):\n",
    "    Y = np.expand_dims(Y, axis = 1)\n",
    "    w = np.random.rand(X.shape[1], 1)\n",
    "    b = 0\n",
    "    \n",
    "    history = []\n",
    "    best_loss = float('+inf')\n",
    "    best_w = None\n",
    "    best_b = None\n",
    "    checkpoint_step = max_iter // 10 if max_iter >= 5 else 1\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        z = np.matmul(X, w)\n",
    "        prediction = sigmoid(z)\n",
    "        loss = loss_function(prediction, Y)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_w = w\n",
    "            best_b = b\n",
    "        \n",
    "        dw, db = de_Logistic(prediction, X, Y)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        history.append(loss)\n",
    "        \n",
    "        if (i + 1) % checkpoint_step == 0:\n",
    "            binary_pred = (prediction >= 0.5) * 1  # 0 or 1\n",
    "            correct = 0\n",
    "            for k in range(len(binary_pred)):\n",
    "                if binary_pred[k] == Y[k]:\n",
    "                    correct += 1\n",
    "            accuracy = correct / len(binary_pred)\n",
    "            print('{} / {} Current loss is {}, Traning accuracy is {}'.format(i + 1, max_iter, loss, accuracy))\n",
    "    return history, best_w, best_b, accuracy\n",
    "\n",
    "# Setosa = 0, Vesicolour and Virginica = 1\n",
    "binary_train_label = np.array([1 if x != 0 else 0 for x in train_label])\n",
    "_, best_w, best_b, _ = Logistic_train_classifier(train_samp, binary_train_label, 0.001, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Implement classification based on hinge loss using GD by implementing the gradient function deHinge(preds, W, X, Y) and optimizing using GD. preds are the prediction from the model, W describes the model parameters, X is the data and Y represents the labels. Propose a function interface for your implementation of the gradient descent algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 / 3000 Current loss is 0.0594060582797743, Training accuracy is 0.9895833333333334\n",
      "600 / 3000 Current loss is 0.04838461716198697, Training accuracy is 0.9895833333333334\n",
      "900 / 3000 Current loss is 0.04049637931476736, Training accuracy is 0.9895833333333334\n",
      "1200 / 3000 Current loss is 0.03493853617260379, Training accuracy is 1.0\n",
      "1500 / 3000 Current loss is 0.03051460640992427, Training accuracy is 1.0\n",
      "1800 / 3000 Current loss is 0.028495786986287668, Training accuracy is 1.0\n",
      "2100 / 3000 Current loss is 0.02791117558777122, Training accuracy is 1.0\n",
      "2400 / 3000 Current loss is 0.02448372618604, Training accuracy is 1.0\n",
      "2700 / 3000 Current loss is 0.022513018312925164, Training accuracy is 1.0\n",
      "3000 / 3000 Current loss is 0.02113107386848015, Training accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "def hinge_loss_function(s, Y):\n",
    "    n_samples = len(s)\n",
    "    correct_score = s[list(range(n_samples)), Y]\n",
    "    correct_score = np.expand_dims(correct_score, axis = 1)  # Save correct score from the labels\n",
    "    \n",
    "    s = s - correct_score + 1  # sj - syi + 1 (syi = correct score)\n",
    "    s[list(range(n_samples)), Y] = 0  # Correct scores to make zero because not into the hinge loss\n",
    "    loss = np.sum(np.fmax(s, 0)) / n_samples\n",
    "    return s, loss\n",
    "\n",
    "def deHinge(preds, w, X, Y):\n",
    "    s = preds\n",
    "    dw = np.zeros(w.shape)\n",
    "    X_mask_for_margin = np.zeros(s.shape)  # n * class\n",
    "    X_mask_for_margin[s > 0] = 1  # positive? -> 1\n",
    "    X_mask_for_margin[np.arange(len(s)), Y] = -np.sum(X_mask_for_margin, axis = 1)  # Counts only positive\n",
    "    dw = X.T.dot(X_mask_for_margin)\n",
    "    dw = dw / len(s)\n",
    "    return dw\n",
    "\n",
    "def Hinge_train_classifier(X, Y, learning_rate, max_iter):\n",
    "    w = np.random.rand(X.shape[1], len(set(Y)))\n",
    "    b = 0\n",
    "    \n",
    "    history = []\n",
    "    best_loss = float('+inf')\n",
    "    best_w = None\n",
    "    checkpoint_step = max_iter // 10 if max_iter >= 5 else 1\n",
    "    for i in range(max_iter):\n",
    "        score = np.matmul(X, w)  # X(N, dims) w(dims, 3)\n",
    "        s, loss = hinge_loss_function(score, Y)\n",
    "        if loss < best_loss:\n",
    "            best_w = w\n",
    "            best_loss = loss\n",
    "            \n",
    "        dw = deHinge(s, w, X, Y)\n",
    "        w = w - learning_rate * dw\n",
    "        history.append(loss)\n",
    "        if (i + 1) % checkpoint_step == 0:\n",
    "            prediction = np.argmax(score, axis = 1)  # Multi-class\n",
    "            correct = 0\n",
    "            for k in range(len(prediction)):\n",
    "                if prediction[k] == Y[k]:\n",
    "                    correct += 1\n",
    "            accuracy = correct / len(prediction)\n",
    "            print(\"{} / {} Current loss is {}, Training accuracy is {}\".format(i + 1, max_iter, loss, accuracy))\n",
    "    return history, best_w\n",
    "\n",
    "_, _ = Hinge_train_classifier(train_samp, train_label, 0.1, 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Implement kernel SVM function ksvm(kernel, X, Y, xtest). The function takes as input a kernel function kernel, training data X, Y and a set of test points xtest. The function returns the set of support vectors along with the predicted labels. You are allowed to use scipy optimization library to solve the quadratic problem of SVM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ksvm(kernel, X, Y, xtest):\n",
    "    support_idx = ksvm.support_\n",
    "    support_vectors = X_train[support_idx]\n",
    "    intercept = ksvm.intercept_\n",
    "    surpport_lb = Y_train[support_idx] * np.abs(ksvm.dual_coef_)\n",
    "    \n",
    "    return support_vectors, support_lb\n",
    "\n",
    "def kernel(xi, xj):\n",
    "    poly_kernel_function = ((np.dot(xi.T, xj) + 1)**d)\n",
    "    return poly_kernel_func\n",
    "    \n",
    "def kernel_rbf(xi, xj):\n",
    "    d1 = xi - xj  # vector\n",
    "    d2 = np.dot(d1, d1)   # vector squared = number\n",
    "    \n",
    "    return np.exp(-d2 / 2)\n",
    "\n",
    "def predict(xj, support_vect, support_lb):\n",
    "    kv = np.apply_along_axis(lambda xi: kernel_rbf(xi, xj), 1, support_vectors)\n",
    "    preds = kv * support_lb\n",
    "    \n",
    "    return np.sum(preds)\n",
    "\n",
    "train_samp /= 255.0\n",
    "train_label /= 255.0\n",
    "\n",
    "support_vecs, support_lb = ksvm(kernel, train_samp, val_samp, train_label)\n",
    "kernel = kernel(support_vecs, support_lb)\n",
    "\n",
    "preds = np.apply_along_axis(lambda x_j : predict(x_j, support_vect, support_lb), 1, X_test[:50] / 255.0)\n",
    "preds = 2 * (preds > 0) - 1\n",
    "plot_confusion_matrix(Y_test[:50], preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
